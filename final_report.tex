\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{hyperref} % For clickable links
\usepackage{amsmath} 

\title{CS598 Project Report}
\author{Michael Xia, Martin Li}
\date{May 2025}

\begin{document}

\maketitle

\section{Abstract}
Link to the video: \href{<insert-video-link-here>}{Video Link} \\
Link to the public Github repository: \href{https://github.com/martin-lzhijian/DLH-Califorest}{GitHub Repository}

\section{Introduction}
The paper “CaliForest: Calibrated Random Forest for Health Data” introduces a method that improves the calibration of probability outputs in random forest classifiers by using out-of-bag (OOB) prediction variance as a measure of predicting its reliability. The authors propose weighting each training sample based on the variance of its OOB predictions and then training a calibration model (either isotonic or logistic regression) to adjust the random forest's raw probabilities. This technique aims to address the common issue where standard random forests produce well-discriminated but poorly calibrated probabilities, especially in healthcare settings.

\subsection*{Reproduction Summary}
Using the authors’ publicly available code, we successfully reproduced the main components of the CaliForest paper:
\begin{itemize}
  \item Loaded and processed the MIMIC-III datasets for ICU mortality.
  \item Implemented the full CaliForest pipeline including OOB variance collection, weighted calibration training, and prediction.
  \item Trained five models across 10 random seeds.
  \item Evaluated performance using AUC, Brier score (scaled and unscaled), Hosmer-Lemeshow test, Spiegelhalter test, and reliability scores.
  \item Saved experiment results to CSV for downstream analysis.
\end{itemize}

\subsection*{Models Evaluated}
Five models were evaluated:
\begin{enumerate}
  \item \textbf{CF-Iso}: CaliForest with isotonic calibration
  \item \textbf{CF-Logit}: CaliForest with logistic calibration
  \item \textbf{RC-Iso}: RC30 with isotonic calibration
  \item \textbf{RC-Logit}: RC30 with logistic calibration
  \item \textbf{RF-NoCal}: Uncalibrated random forest baseline
\end{enumerate}

\subsection*{Datasets Used}
Each model was tested on the following four MIMIC-III datasets:
\begin{itemize}
  \item \texttt{mimic3\_mort\_icu} — ICU mortality
  \item \texttt{mimic3\_mort\_hosp} — hospital mortality
  \item \texttt{mimic3\_los\_3} — length of stay $> 3$ days
  \item \texttt{mimic3\_los\_7} — length of stay $> 7$ days
\end{itemize}

\section{Methodology}

\subsection*{Environment}
?
\subsection*{Data}
?

\subsection*{Model}
CaliForest is a calibrated random forest approach tailored for reliable probability estimation in healthcare. Instead of relying solely on standard out-of-bag (OOB) predictions, it utilizes the variance of predictions across individual trees to quantify uncertainty. These variances are converted into sample weights and fed into a calibration model—either isotonic regression or logistic regression—to enhance calibration while preserving discriminative performance.

\subsection*{Equations and Explanation}

Each prediction is modeled as:

\[
y_i^{\text{val}} = y_i^{\text{oob}} + \varepsilon_i,\quad \varepsilon_i \sim \mathcal{N}(0, \sigma_i^2)
\]

where the OOB prediction $y_i^{\text{oob}}$ is noisy, and $\sigma_i^2$ reflects prediction uncertainty estimated via Bayesian updates using an inverse gamma prior:

\[
\sigma_i^2 = \frac{\beta_0 + s_i^2/2}{\alpha_0 + n_{\text{oob}, i}/2}
\quad\text{with}\quad
w_i = \frac{1}{\sigma_i^2}
\]

These weights $w_i$ are used to train the calibration model on the OOB predictions, effectively prioritizing samples with more confident estimates.

\subsection*{Inputs and Outputs}

\begin{itemize}
  \item \textbf{Inputs:} Feature matrix $X$, label vector $y$
  \item \textbf{Outputs:} Calibrated probability estimates $\hat{p}_i$
  \item \textbf{Technique:} Random forest + OOB variance weighting + calibration via isotonic or logistic regression
\end{itemize}

\section{Training}

\subsection*{Hyperparameters}

Although CaliForest is tree-based rather than neural network-based, it includes several important hyperparameters:

\begin{itemize}
  \item \texttt{n\_estimators = 300} — Number of decision trees in the forest
  \item \texttt{max\_depth = 10} — Maximum depth of each tree
  \item \texttt{min\_samples\_split = 3} — Minimum number of samples required to split an internal node
  \item \texttt{min\_samples\_leaf = 1} — Minimum number of samples required to be at a leaf node
  \item \texttt{ctype = "isotonic"} or \texttt{"logistic"} — Type of calibration model used
\end{itemize}

Note: No learning rate, batch size, or dropout is applicable since this is not a neural architecture.

\subsection*{Computational Requirements}

\begin{itemize}
  \item \textbf{Hardware:} ?
  \item \textbf{Runtime:} ?
  \item \textbf{Total runs:} ?
\end{itemize}

\subsection*{Training Details}

CaliForest fits an ensemble of decision trees using bootstrap aggregation and calibrates their output using either logistic regression or isotonic regression. The training process consists of:

\begin{itemize}
  \item \textbf{Base learners:} Each of the $n=300$ decision trees is trained on a bootstrap sample of the data using Gini impurity.
  \item \textbf{Out-of-bag predictions:} For each sample, predictions are collected from all trees where that sample was not in the training set (OOB).
  \item \textbf{Uncertainty estimation:} The variance of these OOB predictions is used to estimate confidence for each sample.
  \item \textbf{Calibration weights:} Sample weights are computed via a Bayesian update:

  \[
  \alpha_i = \alpha_0 + \frac{n^{\text{oob}}_i}{2}, \quad \beta_i = \beta_0 + \frac{s_i^2}{2}, \quad w_i = \frac{\alpha_i}{\beta_i}
  \]

  where $s_i^2$ is the sample variance of the OOB predictions for the sample $i$. These weights are crucial because they quantify the uncertainty of the OOB predictions. A lower variance (indicating consistent predictions) results in a higher weight, signaling the calibration model to trust the prediction more. In contrast, a higher variance (indicating inconsistency) results in a lower weight, reducing the influence of that prediction during calibration. By incorporating these weights into the calibration model (either isotonic or logistic), CaliForest ensures that more reliable predictions have a greater impact, leading to more robust probability estimation.

  \item \textbf{Calibration model:}
    \begin{itemize}
      \item \textbf{Logistic:} scikit-learn’s \texttt{LogisticRegression} is fit with sample weights $w_i$ to minimize log-loss.
      \item \textbf{Isotonic:} scikit-learn’s \texttt{IsotonicRegression} is trained using the same weights to fit a stepwise non-parametric function.
    \end{itemize}
\end{itemize}

All calibration is performed post-hoc on the aggregated OOB predictions. The final probability output is obtained by averaging predictions from all trees and applying the fitted calibration model.

\subsection*{Evaluations}

To assess both discrimination and calibration, I used the same metrics reported in the original paper:

\begin{itemize}
  \item \textbf{AUC (ROC-AUC)} — \emph{Discrimination} \\
  Measures the model’s ability to rank positive samples higher than negatives. AUC ranges from 0.5 (random) to 1.0 (perfect).
  
  \item \textbf{Brier Score} — \emph{Calibration + accuracy} \\
  Measures the mean squared difference between predicted probabilities and actual binary outcomes. Lower is better.
  
  \item \textbf{Scaled Brier Score} — \emph{Calibration (normalized)} \\
  Normalized Brier Score scaled to the [0, 1] range for interpretability across datasets.
  
  \item \textbf{Hosmer-Lemeshow Test} — \emph{Calibration} \\
  Compares predicted vs. observed outcomes grouped by deciles. High values indicate poor calibration.
  
  \item \textbf{Spiegelhalter Test} — \emph{Calibration (statistical test)} \\
  A statistical test of whether predicted probabilities match observed outcomes.
  
  \item \textbf{Reliability (Small / Large)} — \emph{Calibration visual quality} \\
  Measures the deviation from perfect calibration in low-risk (small) and high-risk (large) probability bins.
\end{itemize}

All metrics were computed using the output from \texttt{model.predict\_proba(X\_test)[:, 1]}, which returns the calibrated probability for the positive class.

\section*{Results}

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{Figure_1.png}

\caption{[In-hospital Mortality] Calibration metrics for the five models with depth = 10 and number of estimators = 300.}

\end{figure}


\subsection*{In-Hospital Mortality Calibration}

Calibration performance across five models was evaluated using six metrics. The results reproduced below align with the original paper’s trends:

\begin{itemize}
\item \textbf{CF-Iso} achieved the best calibration overall:
\begin{itemize}
\item Lowest Brier Score
\item Highest Scaled Brier Score
\item Strongest Hosmer-Lemeshow and Spiegelhalter p-values
\end{itemize}
\item \textbf{RF-NoCal} had the weakest calibration:
\begin{itemize}
\item Highest reliability-in-the-small and poor Spiegelhalter p-values
\end{itemize}
\item \textbf{RC-Iso} and \textbf{RC-Logit} showed more variability and underperformed compared to CaliForest models.
\end{itemize}

\section*{Discussion}

\subsection*{Implications}
The results validate the original paper’s claim: CaliForest improves calibration without compromising discrimination. CF-Iso consistently outperformed other models across multiple metrics, confirming its robustness on real-world clinical tasks.

\subsection*{Reproducibility}
The original paper is reproducible. All code was publicly available, the pipeline (MIMIC-Extract) was well-documented, and results closely matched the paper’s figures.

\subsection*{What Was Easy}
Running the CaliForest package and reproducing calibration metrics using the provided scripts was straightforward.

\subsection*{What Was Difficult}
The original source code did not specify environment configuration, including Python and scikit-learn versions, which required some guesswork to ensure compatibility.

\subsection*{Recommendations}
\begin{itemize}
\item Include an environment specification file (e.g., \texttt{requirements.txt} or \texttt{environment.yml}) to ensure reproducibility across systems.
\item Add more inline comments in the calibration scripts to improve interpretability for first-time users.
\end{itemize}


\end{document}

